{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare line searches to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"ARCHDEFS\"] = f\"{os.path.abspath(os.getcwd())}/CUTEst/ARCHDefs/\"\n",
    "os.environ[\"SIFDECODE\"] = f\"{os.path.abspath(os.getcwd())}/CUTEst/SIFDecode/\"\n",
    "os.environ[\"MASTSIF\"] = f\"{os.path.abspath(os.getcwd())}/CUTEst/sif/\"\n",
    "os.environ[\"CUTEST\"] = f\"{os.path.abspath(os.getcwd())}/CUTEst/CUTEst/\"\n",
    "if sys.platform == \"linux\" or sys.platform == \"linux2\":\n",
    "    # linux\n",
    "    os.environ[\"MYARCH\"] = \"pc64.lnx.gfo\"\n",
    "elif sys.platform == \"darwin\":\n",
    "    # OS X\n",
    "    os.environ[\"MYARCH\"] = \"mac64.osx.gfo\"\n",
    "elif sys.platform == \"win32\":\n",
    "    # Windows...\n",
    "    pass\n",
    "os.environ[\"PYCUTEST_CACHE\"] = (\n",
    "    f\"{os.path.abspath(os.getcwd())}/pycutest_cache_holder/\"\n",
    ")\n",
    "\n",
    "import pycutest\n",
    "\n",
    "allProblemNames = pycutest.find_problems()\n",
    "print(f\"There are {len(allProblemNames)} problems\")\n",
    "\n",
    "bound_problem_names = pycutest.find_problems(constraints=\"bound\")\n",
    "print(f\"There are {len(bound_problem_names)} bound problems\")\n",
    "\n",
    "problemNames = pycutest.find_problems(constraints=\"unconstrained\")\n",
    "print(f\"There are {len(problemNames)} unconstrained problems\")\n",
    "\n",
    "\n",
    "problemName = problemNames[213]\n",
    "problemName = \"CHAINWOO\"\n",
    "print(problemName)\n",
    "problem = pycutest.import_problem(problemName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE_TEST_PATH = os.path.abspath(os.getcwd()) + '/genosolver/'\n",
    "\n",
    "if os.path.exists(HERE_TEST_PATH):\n",
    "    sys.path.insert(0, HERE_TEST_PATH)\n",
    "    from genosolver.bayesopt import line_search_wolfe6 as tomo_line_search\n",
    "    from genosolver.spline import line_search_wolfe4 as cubic_line_search\n",
    "    sys.path.pop(0)\n",
    "else:\n",
    "    raise ImportError('No genosolver folder')\n",
    "\n",
    "HERE_TEST_PATH = os.path.abspath(os.getcwd()) + '/bayesian-geno/'\n",
    "\n",
    "if os.path.exists(HERE_TEST_PATH):\n",
    "    sys.path.insert(0, HERE_TEST_PATH)\n",
    "    from bayesian_line_search.line_search import line_search as bayesian_line_search, LineSearchDebugOptions\n",
    "    sys.path.pop(0)\n",
    "else:\n",
    "    raise ImportError('No bayesian-geno folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "    GENO is a solver for non-linear optimization problems.\n",
    "    It can solve constrained and unconstrained problems.\n",
    "    It is written fully in Python with no dependencies and\n",
    "    can run on the CPU and on the GPU.\n",
    "    It can solve problems of the form:\n",
    "\n",
    "    min_x f(x)\n",
    "    s.t.  cl <= g(x) <= cu\n",
    "          lb <= x <= ub\n",
    "\n",
    "    See https://www.geno-project.org for an easy-to-use interface.\n",
    "\n",
    "\n",
    "    Copyright (C) 2021-2022 Soeren Laue, Mark Blacher\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU Affero General Public License as published\n",
    "    by the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU Affero General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU Affero General Public License\n",
    "    along with this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "    Contact the developer:\n",
    "\n",
    "    E-mail: soeren.laue@uni-jena.de\n",
    "    Web:    https://www.geno-project.org\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "class OptimizeResult(dict):\n",
    "    \"\"\"\n",
    "    Dictionary that returns the result of an optimization process. It is\n",
    "    basically just a copy of the SciPy interface such that they can be used\n",
    "    interchangeably.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError(name) from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.keys():\n",
    "            m = max(map(len, list(self.keys()))) + 1\n",
    "            return '\\n'.join([k.rjust(m) + ': ' + repr(v)\n",
    "                              for k, v in sorted(self.items())])\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "    def __dir__(self):\n",
    "        return list(self.keys())\n",
    "\n",
    "\n",
    "class LBFGSB:\n",
    "    \"\"\"\n",
    "    A quasi-Newton solver for solving bound-constrained optimization problems\n",
    "    of the form\n",
    "\n",
    "        min_x f(x)\n",
    "        s.t.  lb <= x <= ub\n",
    "\n",
    "    It uses the limited memory L-BFGS formula for approximating the\n",
    "    Hessian of f. It avoids the inherently sequential Cauchy-point computation\n",
    "    of the original L-BFGS-B solver and hence, it can be run effciently\n",
    "    on the CPU and on the GPU. The algorithm is described and analyzed in [1].\n",
    "\n",
    "    References:\n",
    "        [1] Soeren Laue, Mark Blacher, and Joachim Giesen.\n",
    "            Optimization for Classical Machine Learning Problems on the GPU.\n",
    "            In AAAI 2022.\n",
    "    \"\"\"\n",
    "    def __init__(self, fg, x0, np, lb=None, ub=None, options=None):\n",
    "        if options is None:\n",
    "            options = {}\n",
    "        self.fg = fg\n",
    "        self.x = x0\n",
    "        self.np = np\n",
    "        self.n = len(self.x)\n",
    "        # self.constrained = not (lb is None and ub is None)\n",
    "        self.lb = np.atleast_1d(lb) if not lb is None else np.full(self.n, -np.inf)\n",
    "        self.ub = np.atleast_1d(ub) if not ub is None else np.full(self.n, np.inf)\n",
    "        self.constrained = not (np.all(self.lb == -np.inf) and np.all(self.ub == np.inf)) \n",
    "        self.set_options(options)\n",
    "        self.init_matrices()\n",
    "        self.working = np.full(self.n, 1.0)\n",
    "        self.eps_f_count = 0\n",
    "\n",
    "    def all_options(self):\n",
    "        return {\n",
    "            \"verbose\", \n",
    "                \"max_iter\", \n",
    "                \"step_max\",\n",
    "                \"max_ls\",\n",
    "                \"eps_pg\", \n",
    "                \"eps_f\", \n",
    "                \"n_eps_f\", \n",
    "                \"m\", \n",
    "                \"grad_test\",\n",
    "                \"callback\",\n",
    "                \"ls\",\n",
    "                \"line_search_debug_options\",\n",
    "                \"max_sample_count\",\n",
    "                }\n",
    "\n",
    "    def set_options(self, options):\n",
    "        unsupported = [opt for opt in options.keys() if opt not in self.all_options()]\n",
    "        for opt in unsupported:\n",
    "            warnings.warn(f\"Option '{opt}' is not supported.\", RuntimeWarning)\n",
    "\n",
    "        self.param = options\n",
    "        self.param.setdefault(\"verbose\", 0)\n",
    "        self.param.setdefault(\"max_iter\", 1000)\n",
    "        self.param.setdefault(\"step_max\", 1e10)\n",
    "        self.param.setdefault(\"max_ls\", 30)\n",
    "        self.param.setdefault(\"eps_pg\", 1E-5)\n",
    "        self.param.setdefault(\"eps_f\", 1E-14)\n",
    "        self.param.setdefault(\"n_eps_f\", 10)\n",
    "        self.param.setdefault(\"m\", 10)\n",
    "        self.param.setdefault(\"grad_test\", False)\n",
    "        self.param.setdefault(\"callback\", None)\n",
    "        self.param.setdefault(\"ls\", 0)\n",
    "        self.param.setdefault(\"line_search_debug_options\", None)\n",
    "        self.param.setdefault(\"max_sample_count\", 20)\n",
    "        self.max_m = self.param['m']\n",
    "\n",
    "    def init_matrices(self):\n",
    "        np = self.np\n",
    "        self.storage_idx = 0\n",
    "        self.S = np.zeros((0, self.n))\n",
    "        self.Y = np.zeros((0, self.n))\n",
    "        self.storage_S = np.empty((2 * self.max_m, self.n))\n",
    "        self.storage_Y = np.empty((2 * self.max_m, self.n))\n",
    "\n",
    "    def add_corrections(self, s, y):\n",
    "        if self.storage_idx >= 2 * self.max_m:\n",
    "            # move everything upfront\n",
    "            self.storage_S[:self.max_m, :] = self.storage_S[self.max_m:, :]\n",
    "            self.storage_Y[:self.max_m, :] = self.storage_Y[self.max_m:, :]\n",
    "            self.storage_idx = self.max_m\n",
    "        self.storage_S[self.storage_idx, :] = s\n",
    "        self.storage_Y[self.storage_idx, :] = y\n",
    "\n",
    "        self.storage_idx += 1\n",
    "        self.S = self.storage_S[max(0, self.storage_idx - self.max_m):self.storage_idx, :]\n",
    "        self.Y = self.storage_Y[max(0, self.storage_idx - self.max_m):self.storage_idx, :]\n",
    "\n",
    "    def force_bounds(self, x):\n",
    "        np = self.np\n",
    "        x = np.maximum(np.minimum(x, self.ub), self.lb)\n",
    "        return x\n",
    "\n",
    "    def proj_grad_norm(self, x, g):\n",
    "        np = self.np\n",
    "        eps = 1E-10\n",
    "        if self.constrained:\n",
    "            g = np.atleast_1d(g)\n",
    "            self.working = np.full(self.n, 1.0)\n",
    "            self.working[(x <= self.lb + eps * 2) & (g >= 0)] = 0\n",
    "            self.working[(x >= self.ub - eps * 2) & (g <= 0)] = 0\n",
    "            pg = np.linalg.norm(g[self.working > 0], np.inf) if any(self.working > 0) else 0.\n",
    "            # pg = np.linalg.norm(np.minimum(np.maximum(x - g, self.lb), self.ub) - x, np.inf)\n",
    "        else:\n",
    "            pg = np.linalg.norm(g, np.inf)\n",
    "        return pg\n",
    "\n",
    "    def max_step_size(self, x, d):\n",
    "        np = self.np\n",
    "        if self.constrained:\n",
    "            step_ub = np.full(self.n, np.inf)\n",
    "            step_lb = np.full(self.n, np.inf)\n",
    "            idx_ub = np.where(d > 0)\n",
    "            idx_lb = np.where(d < 0)\n",
    "            step_ub[idx_ub] = np.divide(self.ub[idx_ub] - x[idx_ub], d[idx_ub])\n",
    "            step_lb[idx_lb] = np.divide(self.lb[idx_lb] - x[idx_lb], d[idx_lb])\n",
    "            step_max = min(np.min(step_ub), np.min(step_lb))\n",
    "        else:\n",
    "            step_max = np.inf\n",
    "        return step_max\n",
    "\n",
    "    def line_search(self, x_old, d, step_max, f_old, g_old):\n",
    "\n",
    "        print()\n",
    "        print(\"Starting BayesianLineSearch\")\n",
    "        bayes_result = bayesian_line_search(\n",
    "            x_old,\n",
    "            d,\n",
    "            self.fg,\n",
    "            step_max,\n",
    "            f_old,\n",
    "            g_old,\n",
    "            quadratic=self.param[\"ls\"] == 2,\n",
    "            np=self.np,\n",
    "            debug_options=self.param[\"line_search_debug_options\"],\n",
    "            max_iter=self.param[\"max_ls\"],\n",
    "            max_sample_count=self.param[\"max_sample_count\"],\n",
    "        )\n",
    "\n",
    "\n",
    "        print()\n",
    "        print(\"Starting TomoLineSearch\")\n",
    "\n",
    "        tomo_result = tomo_line_search(\n",
    "            fg=self.fg,\n",
    "            xk=x_old,\n",
    "            d=d,\n",
    "            g=g_old,\n",
    "            old_fval=f_old,\n",
    "            old_old_fval=None,\n",
    "            c1=1e-4,\n",
    "            c2=0.9,\n",
    "            amax=step_max,\n",
    "            np=self.np,\n",
    "            verbose=self.param[\"verbose\"],\n",
    "        )\n",
    "\n",
    "        print()\n",
    "        print(\"Starting CubicLineSearch\")\n",
    "\n",
    "        cubic_result = cubic_line_search(\n",
    "            fg=self.fg,\n",
    "            xk=x_old,\n",
    "            d=d,\n",
    "            g=g_old,\n",
    "            old_fval=f_old,\n",
    "            old_old_fval=None,\n",
    "            c1=1e-4,\n",
    "            c2=0.9,\n",
    "            amax=step_max,\n",
    "            np=self.np,\n",
    "            verbose=self.param[\"verbose\"],\n",
    "        )\n",
    "\n",
    "        print(\"-----------------Results-----------------\")\n",
    "        f, g, x, step, fg_cnt = bayes_result\n",
    "        print(f\"bayes returned {step} with f:{f} g:{g @ d.T} cnt:{fg_cnt}\")\n",
    "        step, fg_cnt, f, g = tomo_result\n",
    "        print(f\"tomo returned {step} with f:{f} g:{g @ d.T} cnt:{fg_cnt}\")\n",
    "        step, fg_cnt, f, g = cubic_result\n",
    "        print(f\"cubic returned {step} with f:{f} g:{g @ d.T} cnt:{fg_cnt}\")\n",
    "\n",
    "        if bayes_result[0] < tomo_result[2] and bayes_result[0] < cubic_result[2]:\n",
    "            print(\"Using bayes result\")\n",
    "            f, g, x, step, fg_cnt = bayes_result\n",
    "        elif cubic_result[2] < tomo_result[2]:\n",
    "            print(\"Using cubic result\")\n",
    "            step, fg_cnt, f, g = cubic_result\n",
    "        else:\n",
    "            print(\"Using tomo result\")\n",
    "            step, fg_cnt, f, g = tomo_result\n",
    "\n",
    "        if step is None:\n",
    "            x = x_old\n",
    "            f = f_old\n",
    "            g = g_old\n",
    "        else:\n",
    "            x = x_old + step * d\n",
    "            if f is None or g is None:\n",
    "                f, g = self.fg(x)\n",
    "                fg_cnt += 1\n",
    "\n",
    "        return f, g, x, step, fg_cnt\n",
    "\n",
    "    def two_loop(self, g):\n",
    "        np = self.np\n",
    "        k, _ = self.S.shape\n",
    "        rho = np.empty(k)\n",
    "        alpha = np.empty(k)\n",
    "        eps = 1E-40\n",
    "        if self.constrained:\n",
    "            Yw = self.Y * self.working\n",
    "            q = g * self.working\n",
    "        else:\n",
    "            Yw = self.Y\n",
    "            q = g.copy()\n",
    "\n",
    "        if k == 0:\n",
    "            return q\n",
    "\n",
    "        for i in range(k - 1, -1, -1):\n",
    "            rho[i] = np.dot(self.S[i], Yw[i])\n",
    "            yy = np.dot(Yw[i], Yw[i])\n",
    "            if rho[i] > eps * yy:\n",
    "                alpha[i] = np.dot(self.S[i], q) / rho[i]\n",
    "                q -= alpha[i] * Yw[i]\n",
    "\n",
    "        yy = np.dot(Yw[k - 1], Yw[k - 1])\n",
    "        if rho[k - 1] > eps * yy:\n",
    "            gamma = rho[k - 1] / yy\n",
    "            q *= gamma\n",
    "\n",
    "        for i in range(k):\n",
    "            yy = np.dot(Yw[i], Yw[i])\n",
    "            if rho[i] > eps * yy:\n",
    "                beta = np.dot(Yw[i], q) / rho[i]\n",
    "                q += (alpha[i] - beta) * self.S[i]\n",
    "\n",
    "        if self.constrained:\n",
    "            q = q * self.working\n",
    "        return q\n",
    "\n",
    "    def project_direction(self, x, g, d):\n",
    "        np = self.np\n",
    "        eps = 1E-20\n",
    "        x_new = x + d\n",
    "        idx_lb = x_new <= self.lb + 2 * eps\n",
    "        idx_ub = x_new >= self.ub - 2 * eps\n",
    "        x_new[idx_lb] = self.lb[idx_lb]\n",
    "        x_new[idx_ub] = self.ub[idx_ub]\n",
    "        d_new = x_new - x\n",
    "        if np.dot(g, d_new) < -eps:\n",
    "            return d_new\n",
    "\n",
    "        d[(d < 0) & (x <= self.lb + 2 * eps)] = 0\n",
    "        d[(d > 0) & (x >= self.ub - 2 * eps)] = 0\n",
    "\n",
    "        return d\n",
    "\n",
    "    def num_cors(self):\n",
    "        k, _ = self.S.shape\n",
    "        return k\n",
    "\n",
    "    def grad_test(self, x):\n",
    "        np = self.np\n",
    "        t = 1E-6\n",
    "        delta = np.random.randn(self.n)\n",
    "        f1, _ = self.fg(x + t * delta)\n",
    "        f2, _ = self.fg(x - t * delta)\n",
    "        _, g = self.fg(x)\n",
    "        d = (f1 - f2) / (2 * t) - np.dot(g, delta)\n",
    "        if isinstance(d, np.ndarray):\n",
    "            print(f'gradient test: approximation error {d[0]:.5g}')\n",
    "        else:\n",
    "            print(f'gradient test: approximation error {d:.5g}')\n",
    "        return d\n",
    "\n",
    "    def minimize(self):\n",
    "        np = self.np\n",
    "        eps = 1E-40\n",
    "        # check for feasibility\n",
    "        if np.any(self.lb > self.ub):\n",
    "            return OptimizeResult(x=self.x, fun=None, jac=None,\n",
    "                                  nit=0, nfev=0,\n",
    "                                  status=1, success=False,\n",
    "                                  message=\"Infeasible\")\n",
    "        x = self.force_bounds(self.x)\n",
    "        if self.param['grad_test']:\n",
    "            self.grad_test(x)\n",
    "\n",
    "        f, g = self.fg(x)\n",
    "        fun_eval = 1\n",
    "        x_old = x\n",
    "        g_old = g\n",
    "        pg = self.proj_grad_norm(x, g)\n",
    "\n",
    "        # check for early stopping\n",
    "        if pg <= self.param['eps_pg']:\n",
    "            return OptimizeResult(x=x, fun=f, jac=g,\n",
    "                                  nit=0, nfev=fun_eval, status=0, success=True,\n",
    "                                  message=\"Solved\")\n",
    "\n",
    "        # initial direction\n",
    "        d = -g * self.working\n",
    "        d /= np.linalg.norm(d)\n",
    "\n",
    "        if self.param['verbose'] >= 10:\n",
    "            print()\n",
    "            print(\"%9s%9s%15s%15s%15s\" % (\"Iteration\", \"Funeval\",\n",
    "                                                \"Step Length\", \"FunValue\",\n",
    "                                                \"Proj.Grad.\"))\n",
    "\n",
    "        #        f_old = f + np.linalg.norm(g) / 2\n",
    "        f_old = f\n",
    "        x_old = x\n",
    "        k = 0\n",
    "        while True:\n",
    "            k += 1\n",
    "\n",
    "            if not self.param['callback'] is None:\n",
    "                if self.param['callback'](x, f):\n",
    "                    status = 0\n",
    "                    message = \"Callback returned True\"\n",
    "                    break\n",
    "\n",
    "            if self.param['grad_test']:\n",
    "                self.grad_test(x)\n",
    "\n",
    "            step_max = self.max_step_size(x, d)\n",
    "            step_max = min(step_max, 1E10)\n",
    "            if self.param['verbose'] >= 100:\n",
    "                print('lb', self.lb)\n",
    "                print('x', x)\n",
    "                print('ub', self.ub)\n",
    "                print('g', g)\n",
    "                print('d', d)\n",
    "                print('step_max', step_max)\n",
    "\n",
    "            '''\n",
    "            if step_max < 1E-5:\n",
    "                if self.num_cors() > 0:\n",
    "                    # maybe clearing up all correction pairs will help\n",
    "                    if self.param['verbose'] >= 10:\n",
    "                        print('refresh called')\n",
    "                    self.init_matrices()\n",
    "\n",
    "                    # initial direction\n",
    "                    d = -g * self.working\n",
    "                    d /= np.linalg.norm(d)\n",
    "                    f_old =  None\n",
    "                    continue\n",
    "            '''\n",
    "\n",
    "            f_old = f\n",
    "            f, g, x, step, fun_eval_ls = self.line_search(x, d, step_max, f, g)\n",
    "\n",
    "            if f >= f_old:\n",
    "                print('Error, f_new >= f_old: %.15f >= %.15f' % (f, f_old))\n",
    "                print('with step size', step)\n",
    "                step = None\n",
    "\n",
    "            if step is None:\n",
    "                status = 3\n",
    "                message = \"Line search failed\"\n",
    "                warnings.warn(message)\n",
    "\n",
    "            if step is None:\n",
    "\n",
    "                x = x_old\n",
    "                f, g = self.fg(x)\n",
    "                fun_eval += 1\n",
    "                pg = self.proj_grad_norm(x, g)\n",
    "\n",
    "                # self.grad_test(x)\n",
    "                # line search did not converge\n",
    "                if self.num_cors() > 0:\n",
    "                    # maybe clearing up all correction pairs will help\n",
    "                    if self.param['verbose'] >= 10:\n",
    "                        print('refresh called')\n",
    "                    self.init_matrices()\n",
    "\n",
    "                    # initial direction\n",
    "                    d = -g * self.working\n",
    "                    d /= np.linalg.norm(d)\n",
    "                    g_old = g\n",
    "                    f_old = f\n",
    "                    continue\n",
    "                else:\n",
    "                    # really cannot do any progress due to numerical errors\n",
    "                    status = 3\n",
    "                    message = \"Line search failed\"\n",
    "                    break\n",
    "\n",
    "            ###\n",
    "            x = self.force_bounds(x)\n",
    "            pg = self.proj_grad_norm(x, g)\n",
    "            fun_eval += fun_eval_ls\n",
    "\n",
    "            if self.param['verbose'] >= 10:\n",
    "                print(\"%9d%9d%15.5g%15.5E%15.5E\" % (k, fun_eval, step, f, pg))\n",
    "\n",
    "            # check for convergence\n",
    "            if k >= self.param['max_iter']:\n",
    "                f_old = f\n",
    "                x_old = x\n",
    "                g_old = g\n",
    "                status = 2\n",
    "                message = \"Maximum iterations reached\"\n",
    "                break\n",
    "\n",
    "            if pg <= self.param['eps_pg']:\n",
    "                f_old = f\n",
    "                x_old = x\n",
    "                g_old = g\n",
    "                status = 0\n",
    "                message = \"Solved eps_pg\"\n",
    "                break\n",
    "\n",
    "            if (f_old - f) / (np.abs(f) + 1) <= self.param['eps_f']:\n",
    "                self.eps_f_count += 1\n",
    "                if self.eps_f_count >= self.param['n_eps_f']:\n",
    "                    f_old = f\n",
    "                    g_old = g\n",
    "                    x_old = x\n",
    "                    status = 0\n",
    "                    message = \"Solved eps_f\"\n",
    "                    break\n",
    "            else:\n",
    "                self.eps_f_count = 0\n",
    "\n",
    "            s = x - x_old\n",
    "            y = g - g_old\n",
    "\n",
    "            if np.dot(s, y) > eps * np.dot(y, y):\n",
    "                self.add_corrections(s, y)\n",
    "            elif self.param['verbose'] > 100:\n",
    "                print('pair not added:', s, y)\n",
    "            if self.param['verbose'] > 100:\n",
    "                print('S =', self.S)\n",
    "                print('Y =', self.Y)\n",
    "\n",
    "            d = -self.two_loop(g)\n",
    "            dg = np.dot(g, d)\n",
    "            assert dg < 0\n",
    "            d = self.project_direction(x, g, d)\n",
    "            dg = np.dot(g, d)\n",
    "            assert dg < 0\n",
    "\n",
    "            x_old = x\n",
    "            g_old = g\n",
    "\n",
    "        return OptimizeResult(x=x_old, fun=f_old, jac=g_old,\n",
    "                              nit=k, nfev=fun_eval,\n",
    "                              status=status, success=(status==0),\n",
    "                              message=message)\n",
    "\n",
    "\n",
    "class Augmented_Lagrangian_NLP:\n",
    "    def __init__(self, fg, c_f, c_jac, c_lb, c_ub, y, np):\n",
    "        self.np = np\n",
    "        self.fg = fg\n",
    "        self.c_f = c_f\n",
    "        self.c_jac = c_jac\n",
    "        self.c_lb = c_lb\n",
    "        self.c_ub = c_ub\n",
    "        self.rho = None\n",
    "        self.y = y\n",
    "\n",
    "    def constraint_error(self, x):\n",
    "        c = self.c_f(x)\n",
    "        cl = c - self.c_lb\n",
    "        cu = c - self.c_ub\n",
    "        aug_Lag = self.np.minimum(cl + self.y / self.rho, 0.) + \\\n",
    "                  self.np.maximum(cu + self.y / self.rho, 0.)\n",
    "        return cl, cu, aug_Lag\n",
    "\n",
    "    def aug_Lag_fg(self, x):\n",
    "        f, g = self.fg(x)\n",
    "        _, _, aug_Lag = self.constraint_error(x)\n",
    "\n",
    "        f = f + self.rho / 2 * self.np.linalg.norm(aug_Lag) ** 2\n",
    "        v = self.rho * (aug_Lag)\n",
    "        c_g = self.c_jac(x, v)\n",
    "\n",
    "        # No += since we might overwrite the g from genoNLP.\n",
    "        g = g + c_g\n",
    "        return f, g\n",
    "\n",
    "\n",
    "class Augmented_Lagrangian:\n",
    "    \"\"\"\n",
    "    An augmented Lagrangian solver for solving constrained optimization\n",
    "    problems of the form\n",
    "    quasi-Newton solver for solving bound-constrained optimization problems\n",
    "    of the form\n",
    "\n",
    "        min_x f(x)\n",
    "        s.t.  cl <= g(x) <= cu\n",
    "              lb <= x <= ub\n",
    "\n",
    "    If converts the constrained problem into a sequence of bound-constrained\n",
    "    problems and solves them using the L-BFGS-B solver.\n",
    "    \"\"\"\n",
    "    def __init__(self, aug_Lag_NLP, x0, np, lb=None, ub=None, options=None):\n",
    "        if options is None:\n",
    "            options = {}\n",
    "        self.NLP = aug_Lag_NLP\n",
    "        self.x = x0\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.np = np\n",
    "        n, = self.NLP.c_lb.shape\n",
    "        self.y = np.zeros(n)\n",
    "        self.set_options(options)\n",
    "\n",
    "    def set_options(self, options):\n",
    "        all_options = {'verbose', 'max_iter', 'step_max', 'max_ls',\n",
    "                       'eps_pg', 'm', 'grad_test',\n",
    "                       'max_iter_outer', 'constraint_tol'}\n",
    "        unsupported = [opt for opt in options.keys() if opt not in all_options]\n",
    "        for opt in unsupported:\n",
    "            warnings.warn(f\"Option '{opt}' is not supported.\", RuntimeWarning)\n",
    "\n",
    "        self.param = options.copy()\n",
    "        self.param.setdefault('verbose', 0)\n",
    "        self.param.setdefault('max_iter_outer', 100)\n",
    "        self.param.setdefault('constraint_tol', 1E-3)\n",
    "\n",
    "        self.param_LBFGSB = options.copy()\n",
    "        self.param_LBFGSB.pop('max_iter_outer', None)\n",
    "        self.param_LBFGSB.pop('constraint_tol', None)\n",
    "\n",
    "    def minimize(self):\n",
    "        np = self.np\n",
    "        rho = 1.0\n",
    "        k = 0\n",
    "        fun_eval = 0\n",
    "        n_inner = 0\n",
    "        jac = 0\n",
    "        constraint_error_old = np.inf\n",
    "        rho_increases = 0\n",
    "        while True:\n",
    "            k += 1\n",
    "            if self.param['verbose'] >= 5:\n",
    "                print('outer iteration', k)\n",
    "                print('rho', rho)\n",
    "                print('y', self.y)\n",
    "\n",
    "            self.NLP.rho = rho\n",
    "            self.NLP.y = self.y\n",
    "            solver = LBFGSB(self.NLP.aug_Lag_fg, self.x, np, self.lb, self.ub, self.param_LBFGSB)\n",
    "            res = solver.minimize()\n",
    "\n",
    "            self.x = res.x\n",
    "            fun_eval += res.nfev\n",
    "            n_inner += res.nit\n",
    "            jac = res.jac\n",
    "\n",
    "            cl, cu, aug_constraint_error = self.NLP.constraint_error(self.x)\n",
    "            constraint_error = np.minimum(cl, 0.) + np.maximum(cu, 0.)\n",
    "            if self.param['verbose'] >= 90:\n",
    "                print('x', self.x)\n",
    "                print('c_f', self.NLP.c_f(self.x))\n",
    "                print('cl', cl)\n",
    "                print('cu', cu)\n",
    "            if self.param['verbose'] >= 5:\n",
    "                print('constraint_error', constraint_error)\n",
    "\n",
    "            constraint_error_norm = np.linalg.norm(constraint_error, np.inf)\n",
    "            if constraint_error_norm < self.param['constraint_tol']:\n",
    "                status = res.status\n",
    "                message = res.message\n",
    "                break\n",
    "\n",
    "            if res.status==1: # augmented Lagrangian could not be solved\n",
    "                status = 1\n",
    "                message = \"Infeasible\"\n",
    "                break\n",
    "\n",
    "            if k >= self.param['max_iter_outer']:\n",
    "                status = 2\n",
    "                message = \"Maximum outer iterations reached\"\n",
    "                break\n",
    "\n",
    "            self.y = rho * aug_constraint_error\n",
    "            if constraint_error_norm > constraint_error_old * 0.5:\n",
    "                rho *= 2\n",
    "                rho_increases += 1\n",
    "            else:\n",
    "                rho_increases = 0\n",
    "            constraint_error_old = constraint_error_norm\n",
    "\n",
    "            if rho_increases > 20:\n",
    "                status = 1\n",
    "                message = \"Infeasible\"\n",
    "                break # problem seems infeasible\n",
    "\n",
    "        f, _ = self.NLP.fg(self.x)\n",
    "        return OptimizeResult(x=self.x, y=self.y, fun=f, jac=jac,\n",
    "                              nit=k, nfev=fun_eval, nInner=n_inner,\n",
    "                              maxcv=constraint_error_norm,\n",
    "                              slack=0,\n",
    "                              status=status, success=(status==0),\n",
    "                              message=message)\n",
    "\n",
    "\n",
    "def minimize(fg, x0, lb=None, ub=None, options=None, constraints=None, np=None):\n",
    "    if np is None:\n",
    "        import numpy as np\n",
    "    if options is None:\n",
    "        options = {}\n",
    "    x0 = np.ascontiguousarray(np.array(x0))\n",
    "    if not lb is None:\n",
    "        lb = np.ascontiguousarray(np.array(lb))\n",
    "    if not ub is None:\n",
    "        ub = np.ascontiguousarray(np.array(ub))\n",
    "    if not constraints:\n",
    "        solver = LBFGSB(fg, x0, np, lb, ub, options)\n",
    "    else:\n",
    "        if isinstance(constraints, dict):\n",
    "            constraints = (constraints, )\n",
    "\n",
    "        shape_constraints = []\n",
    "        offset = [0]\n",
    "        c_lb_all = []\n",
    "        c_ub_all = []\n",
    "        for c in constraints:\n",
    "            # determine shape of constraint i and its length\n",
    "            dummy_f_c = c['fun'](x0)\n",
    "            shape_constraints.append(dummy_f_c.shape)\n",
    "            m = len(dummy_f_c.reshape(-1))\n",
    "            offset.append(offset[-1] + m)\n",
    "            try:\n",
    "                c_lb = c['lb']\n",
    "                c_ub = c['ub']\n",
    "            except KeyError:\n",
    "                # old\n",
    "                # because of backward compatibility\n",
    "                # check the type of constraints\n",
    "                if c['type'] == 'eq':\n",
    "                    c_lb = np.zeros(m)\n",
    "                    c_ub = np.zeros(m)\n",
    "                elif c['type'] == 'ineq':\n",
    "                    c_lb = np.full(m, -np.inf)\n",
    "                    c_ub = np.zeros(m)\n",
    "                else:\n",
    "                    assert False\n",
    "            \n",
    "            c_lb_all.append(c_lb)\n",
    "            c_ub_all.append(c_ub)\n",
    "\n",
    "        mTotal = offset[-1]\n",
    "        c_lb_all = np.concatenate(c_lb_all)\n",
    "        c_ub_all = np.concatenate(c_ub_all)\n",
    "\n",
    "        def c_f_all(x):\n",
    "            l = [c['fun'](x).reshape(-1) for c in constraints]\n",
    "            f = np.concatenate(l)\n",
    "            return f\n",
    "        def c_jac_all(x, v):\n",
    "            g = np.zeros_like(x)\n",
    "            for i, c in enumerate(constraints):\n",
    "                g = g + c['jacprod'](x, v[offset[i]:offset[i+1]].reshape(shape_constraints[i])).reshape(-1)\n",
    "            return g\n",
    "\n",
    "        y = np.zeros(mTotal)\n",
    "\n",
    "        augmented_Lagrangian_NLP = Augmented_Lagrangian_NLP(fg, c_f_all, c_jac_all,\n",
    "                                                            c_lb_all, c_ub_all, y, np)\n",
    "        solver = Augmented_Lagrangian(augmented_Lagrangian_NLP, x0, np, lb, ub, options)\n",
    "    return solver.minimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fandg = lambda x: problem.obj(x, gradient=True)\n",
    "\n",
    "do_debug = False\n",
    "debug_options = LineSearchDebugOptions(\n",
    "    report_termination_reason=do_debug,\n",
    "    report_wolfe_termination=True,\n",
    "    report_return_value=do_debug,\n",
    "    report_insufficient_acquisition=do_debug,\n",
    "    report_invalid_f=do_debug,\n",
    "    report_acquisition_max=False,\n",
    "    report_area_reduction=True,\n",
    "    report_kernel_hyperparameter=False,\n",
    "    gp_verbose=False,\n",
    "    plot_gp=do_debug,\n",
    "    plot_threshold=np.inf,\n",
    ")\n",
    "\n",
    "\n",
    "options = {\n",
    "        \"eps_pg\": 1e-8,  # Gradient for early stop\n",
    "        \"max_iter\": 3000,\n",
    "        \"verbose\": 10,  # Set it to 0 to fully mute it.\n",
    "        \"max_ls\": 50,\n",
    "        \"line_search_debug_options\": debug_options,\n",
    "        # \"max_sample_count\": 20,\n",
    "    }\n",
    "\n",
    "constraints = None\n",
    "\n",
    "minimize(\n",
    "    fandg, problem.x0, lb=problem.bl, ub=problem.bu, options=options, constraints=constraints, np=np\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
